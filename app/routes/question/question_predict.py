from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, PreTrainedTokenizerFast
from konlpy.tag import Okt
import os, json, torch, re, random
from datetime import datetime
from database import get_connection
from fastapi.responses import JSONResponse
import traceback

router = APIRouter()

# ìš”ì²­ ë°ì´í„° í¬ë§·
class TextInput(BaseModel):
    paragraph: str

# í˜•íƒœì†Œ ë¶„ì„ê¸°
okt = Okt()

# ê²½ë¡œ ì„¤ì •
CURRENT_DIR = os.path.dirname(__file__)
PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, "..", "..", ".."))
TEMPLATE_PATH = os.path.join(PROJECT_ROOT, "data", "question", "processed", "question_data.json")
CONFIG_PATH = os.path.join(PROJECT_ROOT, "app", "models", "question", "question_model_run.json")
METRICS_PATH = os.path.join(PROJECT_ROOT, "data", "question", "question_model_metrics.json")
MODELS_DIR = os.path.join(PROJECT_ROOT, "models", "question")

# í…œí”Œë¦¿ ë¡œë“œ
with open(TEMPLATE_PATH, encoding="utf-8") as f:
    template_data = json.load(f)

# ëª¨ë¸ ë¡œë”©
def load_model_and_tokenizer():
    if not os.path.exists(CONFIG_PATH):
        raise HTTPException(status_code=500, detail="question_model_run.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    with open(CONFIG_PATH, encoding='utf-8') as f:
        model_info = json.load(f)
    model_name = model_info[0]['model_name'] if isinstance(model_info, list) else model_info['model_name']
    model_path = os.path.join(MODELS_DIR, model_name)
    if not os.path.exists(os.path.join(model_path, "config.json")):
        raise HTTPException(status_code=500, detail=f"ëª¨ë¸ ë””ë ‰í† ë¦¬ {model_path}ì— HuggingFace ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
    return tokenizer, model

# ì§ˆë¬¸ ìˆ˜ ì¡°ì ˆ
def get_question_count():
    if not os.path.exists(CONFIG_PATH) or not os.path.exists(METRICS_PATH):
        return 2
    try:
        with open(CONFIG_PATH, encoding="utf-8") as f:
            model_info = json.load(f)
        current_model = model_info[0]['model_name'] if isinstance(model_info, list) else model_info['model_name']
        with open(METRICS_PATH, encoding="utf-8") as f:
            metrics = json.load(f)
        for entry in metrics:
            if entry['model_name'] == current_model and 'q_num' in entry:
                return entry['q_num']
    except:
        pass
    return 2

# ë¶ˆìš©ì–´ ì œê±°
def clean_keywords(keywords):
    stopwords = {
        "ê²ƒ", "ì •ë§", "ì§„ì§œ", "ê·¸ëƒ¥", "ì´ëŸ°", "ì €ëŸ°", "ë„ˆë¬´", "ë§¤ìš°", "ì¢€", "ê±°ì˜",
        "ë‚˜", "ë„ˆ", "ìš°ë¦¬", "ì €", "ê·¸", "ì´", "ìœ„", "ì•„ë˜", "ë•Œ", "ì¤‘", "ìˆ˜", "ë“±",
        "ê·¸ë¦¬ê³ ", "ê·¸ë˜ì„œ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ê·¸ë•Œ", "ìš”ì¦˜", "ì˜¤ëŠ˜", "ë‚´ì¼"
    }
    return [kw.strip() for kw in keywords if kw.strip() not in stopwords and len(kw.strip()) > 1]

# âœ… ìš”ì•½ í•¨ìˆ˜ (ì¶œë ¥ í¬í•¨)
def summarize_kobart(text):
    tokenizer, model = load_model_and_tokenizer()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.eval()
    model.to(device)
    input_ids = tokenizer.encode(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512
    ).to(device)
    output_ids = model.generate(
        input_ids,
        max_length=128,
        num_beams=4,
        early_stopping=True
    )
    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    # âœ… ìš”ì•½ ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“˜ ìš”ì•½ ê²°ê³¼:\n{summary}\n")
    return summary

# í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords_okt(text, top_k=5):
    nouns = okt.nouns(text)
    cleaned = clean_keywords(nouns)
    return cleaned[:top_k]

# ì¡°ì‚¬ ë³´ì •
def adjust_postposition(keyword, template):
    last_char = keyword[-1]
    try:
        code = ord(last_char)
        has_final = (code - 44032) % 28 != 0 if 0xAC00 <= code <= 0xD7A3 else False
    except:
        has_final = False

    postpositions = {
        r"\(ì´\)ê°€": "ì´" if has_final else "ê°€",
        r"\(ì„\)ë¥¼": "ì„" if has_final else "ë¥¼",
        r"\(ì€\)ëŠ”": "ì€" if has_final else "ëŠ”",
        r"\(ê³¼\)ì™€": "ê³¼" if has_final else "ì™€",
        r"\(ì´\)ë€": "ì´ë€" if has_final else "ë€",
        r"\(ì—\)": "ì—"
    }
    for pattern, replacement in postpositions.items():
        template = re.sub(pattern, replacement, template)

    return template.replace("(í‚¤ì›Œë“œ)", keyword).replace("  ", " ").strip()

# ì§ˆë¬¸ ìƒì„±
def generate_questions_from_template(summary, target_count=2):
    keywords = extract_keywords_okt(summary)
    candidates = [q['template'] for q in template_data["questions"] if "(í‚¤ì›Œë“œ)" in q['template']]
    random.shuffle(keywords)
    questions = []
    for kw in keywords:
        if not candidates:
            break
        template = random.choice(candidates)
        questions.append(adjust_postposition(kw, template))
        if len(questions) >= target_count:
            break
    if len(questions) < target_count:
        raise HTTPException(status_code=500, detail=f"ì§ˆë¬¸ì´ {target_count}ê°œ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return questions

# FastAPI ë¼ìš°íŠ¸
@router.post("/predict_question")
def predict(input_data: TextInput):
    paragraph = " ".join(input_data.paragraph.split())
    if len(paragraph) <= 30:
        raise HTTPException(status_code=422, detail="ë¬¸ì¥ì´ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.")
    try:
        q_num = get_question_count()
        summary = summarize_kobart(paragraph)
        questions = generate_questions_from_template(summary, target_count=q_num)

        date_str = datetime.now().strftime("%Y-%m-%d")
        output_text = " / ".join(questions)

        # DB ì €ì¥
        conn = get_connection()
        try:
            with conn.cursor() as cursor:
                sql = "INSERT INTO questionData (date, input, output) VALUES (%s, %s, %s)"
                cursor.execute(sql, (date_str, paragraph, summary))
            conn.commit()
        finally:
            conn.close()

        # âœ… summaryë„ ì‘ë‹µì— í¬í•¨í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ë¡œ ë³€ê²½ ê°€ëŠ¥
        return JSONResponse(content={
            "summary": summary,
            **{f"question{i+1}": q for i, q in enumerate(questions)}
        })

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
