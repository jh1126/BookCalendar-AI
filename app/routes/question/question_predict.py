from fastapi import APIRouter, HTTPException
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter, defaultdict
from transformers import (
    AutoModel, AutoTokenizer, AutoModelForSeq2SeqLM,
    PreTrainedTokenizerFast, BartForConditionalGeneration
)
from pydantic import BaseModel
import torch.nn.functional as F
import re, random, json, torch, os
from datetime import datetime
from database import get_connection
import traceback
from fastapi.responses import JSONResponse

router = APIRouter()

#ë…í›„ê° input ë°ì´í„°
class TextInput(BaseModel):
    paragraph: str

okt = Okt()

CURRENT_DIR = os.path.dirname(__file__)
PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, "..", "..", ".."))

TEMPLATE_PATH = os.path.join(PROJECT_ROOT, "data", "question", "processed", "question_data.json")
CONFIG_PATH = os.path.join(PROJECT_ROOT, "app", "models", "question", "question_model_run.json")
METRICS_PATH = os.path.join(PROJECT_ROOT, "data", "question", "question_model_metrics.json")
MODELS_DIR = os.path.join(PROJECT_ROOT, "models", "question")



#ì§ˆë¬¸ í…œí”Œë¦¿ ë¡œë”©
with open(TEMPLATE_PATH, encoding="utf-8") as f:
    template_data = json.load(f)


#ë””ë°”ì´ìŠ¤ ì„¤ì • - cudaë¥¼ í•˜ë“œì½”ë”© x, device ë³€ìˆ˜ì— í†µì¼ ì²˜ë¦¬
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



#ì¹´í…Œê³ ë¦¬-í‚¤ì›Œë“œ ì‚¬ì „ ( ì§ˆë¬¸ ìƒì„± ì‹œ í‚¤ì›Œë“œ ì˜ë¯¸ì  ë¶„ë¥˜ ê¸°ì¤€ 'ìƒì‹¤'-> ê°ì •íƒêµ¬ )
category_keywords = {
    "ê°ì •íƒêµ¬": [
        "ê°ì •", "ëŠë‚Œ", "ë§ˆìŒ", "ìƒì‹¤", "ê¸°ì¨", "ë¶„ë…¸", "ë‘ë ¤ì›€", "ì™¸ë¡œì›€", "ì„¤ë ˜", "ë¶ˆì•ˆ", 
        "ì•ˆë„", "í¬ë§", "ì ˆë§", "ì‚¬ë‘", "ì¦ì˜¤", "ìš°ìš¸", "ì—´ì •", "ìˆ˜ì¹˜ì‹¬", "í˜ì˜¤", "ë†€ëŒ"
    ],
    "ê´€ì ì „í™˜": [
        "ì‹œì„ ", "ë°˜ëŒ€", "ë‹¤ë¦„", "ì°¨ì´", "ê²½ê³„", "ê´€ì ", "ì…ì¥", "ë°”ë¼ë³´ê¸°", "ì‹œì•¼", 
        "ë¹„ìœ ", "í¸ê²¬í•´ì†Œ", "ë‹¤ê°ë„", "ë¹„íŒì ì‹œê°", "ë‹¤ë¥¸ìƒê°", "ìƒê°ì˜ìœ ì—°ì„±"
    ],
    "ë©”íƒ€ì¸ì§€": [
        "ê¹¨ë‹¬ìŒ", "ì„±ì°°", "ë˜ëŒì•„ë´„", "ì¸ì§€", "ìƒê°", "ìê°", "ë‚´ë©´", "ë°˜ì„±", "ì •ì‹ ì ì„±ì¥", 
        "ê°ê´€í™”", "ì˜ì‹", "ìê¸°ì´í•´", "ì‚¬ê³ ì˜ê³¼ì •", "ìŠ¤ìŠ¤ë¡œ", "ì§€ê°ë³€í™”"
    ],
    "ë¹„íŒì ì‚¬ê³ ": [
        "ê°ˆë“±", "ë¬¸ì œ", "ê´€ìŠµ", "í¸ê²¬", "ë…¼ë¦¬", "í˜„ì‹¤", "ì˜ë¬¸", "ì˜ì‹¬", "ë°˜ë°•", 
        "ë¶„ì„", "ë…¼ìŸ", "ë¹„íŒ", "í—ˆì ", "ì¶”ë¡ ", "ì‚¬ì‹¤í™•ì¸"
    ],
    "ìƒìƒë ¥ë°œíœ˜": [
        "ìš°ì£¼", "ìƒìƒ", "ê¿ˆ", "ë¯¸ë˜", "ì°½ì˜", "í™˜ìƒ", "ê°€ìƒ", "ë¹„í˜„ì‹¤", "ì´ìƒí–¥", 
        "ê¸°ë°œí•¨", "ëª¨í—˜", "ë³€í˜•", "ë°œëª…", "ë‹¤ë¥¸ì„¸ê³„", "ê³µìƒ"
    ],
    "ì‹œëŒ€ì™€ë§¥ë½": [
        "ê³¼ê±°", "ì—­ì‚¬", "ì‹œëŒ€", "ë¬¸í™”", "ë§¥ë½", "ì „í†µ", "ì„¸ëŒ€", "ì‹œê¸°", "ë°°ê²½", 
        "ì§„ë³´", "ë³´ìˆ˜", "ì‹œëŒ€ì •ì‹ ", "ì‚¬íšŒë³€í™”", "íë¦„", "ì‹œëŒ€ìƒ"
    ],
    "ì‹¬ì¸µì£¼ì œíŒŒê³ ë“¤ê¸°": [
        "ë³¸ì§ˆ", "í•µì‹¬", "ì¤‘ì‹¬", "ì£¼ì œ", "ì˜ë¯¸", "ë‚´í¬", "ê·¼ì›", "ì˜ë„", "êµ¬ì¡°", 
        "ê¹Šì´", "ì¤‘ìš”ì„±", "ì£¼ì œì˜ì‹", "í•µì‹¬ì§ˆë¬¸", "í•µì‹¬ì‚¬ìƒ", "ì´ˆì "
    ],
    "ì—°ê²°ì„±ì°¾ê¸°": [
        "ê´€ê³„", "ì—°ê²°", "ê´€ë ¨", "ë¹„êµ", "ìœ ì‚¬", "ëŒ€ì¡°", "ìƒê´€", "ì ‘ì ", "ìœ ì¶”", 
        "ë§¥ë½", "ì—°ì†ì„±", "ìœ ê¸°ì„±", "í•¨ì˜", "ìƒí˜¸ì‘ìš©", "ì›ì¸ê²°ê³¼"
    ],
    "ìœ¤ë¦¬ì ê³ ë¯¼": [
        "ìœ¤ë¦¬", "ì„ ì•…", "ì„ íƒ", "ê°€ì¹˜", "íŒë‹¨", "ì±…ì„", "ì–‘ì‹¬", "ë„ë•", "ê·œë²”", 
        "ì˜³ê³ ê·¸ë¦„", "ê°ˆë“±ìƒí™©", "ì •ì˜", "ì¸ê°„ë‹¤ì›€", "ì‚¬íšŒê·œë²”", "ê³µì •ì„±"
    ],
    "ì¸ë¬¼ì‹¬ì¸µë¶„ì„": [
        "ì¸ë¬¼", "ì„±ê²©", "í–‰ë™", "ë™ê¸°", "ì„±ì¥", "ë³€í™”", "ë°°ê²½", "ê´€ê³„", "ë§íˆ¬", 
        "ì‹¬ë¦¬", "íŠ¸ë¼ìš°ë§ˆ", "ìš•ë§", "ê²°ì •", "ì—­í• ", "í•œê³„"
    ],
    "ì¥ë¥´ë¶„ì„": [
        "íŒíƒ€ì§€", "ì¶”ë¦¬", "ë¡œë§¨ìŠ¤", "SF", "ì„œì‚¬", "í˜•ì‹", "í”Œë¡¯", "í´ë¦¬ì…°", "ì¥ë¥´íŠ¹ì„±", 
        "ì¥ë¥´ê·œì¹™", "ì„œì‚¬ë°©ì‹", "ì„œë¸Œì¥ë¥´", "ë¬¸ì²´", "ë¶„ìœ„ê¸°", "ì¥ë¥´í•´ì²´"
    ],
    "ì°½ì˜ì ì¬í•´ì„": [
        "ì¬í•´ì„", "ë‹¤ì‹œë³´ê¸°", "ì˜ì™¸ì„±", "ë°˜ì „", "ì°½ì¡°", "ì¬êµ¬ì„±", "í•´ì²´", "ìƒˆë¡œìš´ì‹œê°", 
        "ê¸°ì¡´í‹€ê¹¨ê¸°", "ì‘ìš©", "ë…ì°½ì„±", "íƒˆêµ¬ì¡°", "ë°œëª…", "ë‹¤ì‹œì“°ê¸°", "ì „ë³µ"
    ],
    "í•µì‹¬ê°€ì¹˜": [
        "ììœ ", "ì‚¬ë‘", "ì¡´ì¤‘", "ì±…ì„", "ê³µê°", "ì—°ëŒ€", "ë°°ë ¤", "ìš©ê¸°", "í‰ë“±", 
        "ìì•„", "ììœ¨", "ì •ì˜", "ì •ì§", "ì§„ì‹¤", "ì‹ ë¢°"
    ],
    "í–‰ë™ìœ ë„": [
        "ì‹¤ì²œ", "í–‰ë™", "ë„ì „", "ì°¸ì—¬", "ë³€í™”", "ê²°ë‹¨", "í•´ê²°", "ê³„íš", "ìŠµê´€", 
        "ë¦¬ë”ì‹­", "ì˜í–¥ë ¥", "ìê¸°ì£¼ë„", "ì‚¬íšŒì°¸ì—¬", "ì˜ì§€", "ì‹¤í–‰"
    ]
}





#KoBART ëª¨ë¸ ë¡œë”©
def load_kobart_model_and_tokenizer():
    # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
    with open(CONFIG_PATH, encoding='utf-8') as f:
        model_info = json.load(f)
    model_name = model_info[0]['model_name'] if isinstance(model_info, list) else model_info['model_name']

    # ëª¨ë¸ ê²½ë¡œ êµ¬ì„±
    model_path = os.path.join(MODELS_DIR, model_name)

    # run.json ëª¨ë¸ì˜ Tokenizer, Model ë¡œë“œ(.eval()ì„ í†µí•´ ì¶”ë¡  ëª¨ë“œ)
    #Fastapi ì¬ì‹œì‘ ì—†ì´ ëª¨ë¸ êµì²´ ê°€ëŠ¥
    tokenizer = PreTrainedTokenizerFast(
        tokenizer_file=os.path.join(model_path, "tokenizer.json"),
        bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>'
    )
    model = BartForConditionalGeneration.from_pretrained(model_path).to(device).eval()

    return tokenizer, model


#SBERT ëª¨ë¸ ë¡œë”© ë° ì„¤ì • - ì§ˆë¬¸ í…œí”Œë¦¿ ìœ ì‚¬ë„ ë¹„êµì— ì´ìš©
#í…œí”Œë¦¿ê³¼ í‚¤ì›Œë“œ ê°„ ë¬¸ì¥ ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ì¸¡ì • ì§€ì›
model_name = "snunlp/KR-SBERT-V40K-klueNLI-augSTS"
tokenizer = AutoTokenizer.from_pretrained(model_name)
sbert_model = AutoModel.from_pretrained(model_name)
sbert_model.to("cuda")
sbert_model.eval()


#SBERT ì„ë² ë”© ì¶”ì¶œ í•¨ìˆ˜
#TEXTë¥¼ í† í¬ë‚˜ì´ì§• -> sbertì— ì…ë ¥ -> í† í° ë°±í„° ì¶”ì¶œ
def get_sbert_embedding(text):
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512
    ).to(device)  # âœ… í•˜ë“œì½”ë”©ëœ "cuda" â†’ device ë³€ìˆ˜ë¡œ í†µì¼

    with torch.no_grad():
        outputs = sbert_model(**inputs)

    #normalizeë¥¼ ì ìš©í•´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì— ì í•©í•œ ë‹¨ìœ„ ë²¡í„°ë¡œ ë³€í™˜
    cls_emb = outputs.last_hidden_state[:, 0, :]  # shape: (1, hidden_dim)
    return F.normalize(cls_emb, p=2, dim=1).squeeze(0).to(device)  # âœ… .to(device) ì¶”ê°€ë¡œ ëª…ì‹œ



# 1. (í‚¤ì›Œë“œ) ì œê±°í•œ í…œí”Œë¦¿ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
template_texts_clean = [
    q["template"].replace("(í‚¤ì›Œë“œ)", "").strip()
    for q in template_data.get("questions", [])
]

# 2. SBERT ì„ë² ë”© ì¶”ì¶œ í›„ ìŠ¤íƒ (GPUë¡œ ì´ë™)
#template_embaddingsì— ì €ì¥ ë¼ í•œ ë²ˆë§Œ ì„ë² ë”©í•˜ê³ , í‚¤ì›Œë“œ ì„ë² ë”©, ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë§Œ ê³„ì‚°
template_embeddings = torch.stack([
    get_sbert_embedding(text) for text in template_texts_clean
]).to("cuda" if torch.cuda.is_available() else "cpu")

#ìš”ì•½ ì „ë¦¬ (ì¤„ë°”ê¿ˆ ì œê±°, ê³µë°± ì •ë¦¬)
def preprocess_paragraph(text):
    return ' '.join(text.strip().split())


#ìš”ì•½ í•¨ìˆ˜
kobart_tokenizer, kobart_model = load_kobart_model_and_tokenizer()
def summarize_kobart(text):
    input_ids = kobart_tokenizer.encode(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512
    ).to(device)

    output_ids = kobart_model.generate(
        input_ids,
        max_length=100,
        num_beams=4, #ë‹¤ì–‘í•œ í›„ë³´ë¥¼ ë´„
        early_stopping=True
    )
    #ë””ì½”ë”© : ìˆ«ì IDë¥¼ ë‹¤ì‹œ í•œê¸€ ë¬¸ì¥ìœ¼ë¡œ ë°”ê¿ˆ
    return kobart_tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()

#ì¡°ì‚¬ ë³´ì • í•¨ìˆ˜
def adjust_postposition(keyword, template):
    #keyword ë§ˆì§€ë§‰ ê¸€ì ë°›ì¹¨ ìœ ë¬´ íŒë³„
    def has_jongseong(char):
        code = ord(char)
        return (code - 0xAC00) % 28 != 0 if 0xAC00 <= code <= 0xD7A3 else False

    #íŒë³„í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í…œí”Œë¦¿ ë‚´ ì¡°ì‚¬ ìë™ ì¡°ì •
    has_final = has_jongseong(keyword[-1])
    replacements = {
        r"\(ì´\)ê°€": "ì´" if has_final else "ê°€",
        r"\(ì„\)ë¥¼": "ì„" if has_final else "ë¥¼",
        r"\(ì€\)ëŠ”": "ì€" if has_final else "ëŠ”",
        r"\(ê³¼\)ì™€": "ê³¼" if has_final else "ì™€",
        r"\(ì´\)ë¼ê³ ": "ì´ë¼ê³ " if has_final else "ë¼ê³ ",
    }

    #í‚¤ì›Œë“œë¥¼ ì‹¤ì œ í‚¤ì›Œë“œë¡œ ë‹¤ì²´ í›„ ë°˜í™˜
    for pattern, repl in replacements.items():
        template = re.sub(pattern, repl, template)
    return template.replace("(í‚¤ì›Œë“œ)", keyword).strip()




# ì£¼ì–´ ë³´ì • í•¨ìˆ˜
def clarify_subject(question, keyword):
    abstract_keywords = {"ê°ì •", "ë‚´ë©´", "ì˜ë¯¸", "ìƒê°", "ì¡´ì¬", "ì„±ì°°", "ëŠë‚Œ"}
    human_keywords = {"ì €ì", "ì£¼ì¸ê³µ", "ì¸ë¬¼", "ì‚¬ëŒ"}

    replacement = None

    #ë¬¸ì¥ ë‚´ íŠ¹ì • ë¬¸êµ¬ê°€ ìˆì„ ë•Œ ë³´ì™„
    if any(phrase in question for phrase in ["í•˜ê³  ì‹¶ì–´", "ë§Œë“¤ê³  ì‹¶ì–´", "ëŠë¼ê³  ì‹¶ì–´", "ìƒê°í•˜ë‚˜ìš”", "ì¤‘ìš”í•œê°€ìš”"]):
        if keyword in abstract_keywords:
            replacement = f"ì €ìì˜ {keyword}"
        elif keyword in human_keywords:
            replacement = f"{keyword} ë³¸ì¸ì€"

    # ë‹¨ì–´ê°€ í¬í•¨ëœ ê²½ìš°, ì²˜ìŒ í•œ ë²ˆë§Œ ëŒ€ì²´
    if replacement:
        question = re.sub(rf"\b{re.escape(keyword)}\b", replacement, question, count=1)

    # "ë¬´ì—‡ì¸ê°€ìš”?" â†’ "ì–´ë–¤ ì˜ë¯¸ì¸ê°€ìš”?" ë¡œ ë” ìì—°ìŠ¤ëŸ½ê²Œ
    if "ë¬´ì—‡ì¸ê°€ìš”?" in question and keyword in abstract_keywords:
        question = question.replace("ë¬´ì—‡ì¸ê°€ìš”?", "ì–´ë–¤ ì˜ë¯¸ì¸ê°€ìš”?")

    return question


# ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ ì œê±°
def clean_keywords(keywords):
    stopwords = {
        "ê²ƒ", "ì •ë§", "ì§„ì§œ", "ê·¸ëƒ¥", "ì´ëŸ°", "ì €ëŸ°", "ë„ˆë¬´", "ë§¤ìš°", "ì¢€", "ê±°ì˜",
        "ë‚˜", "ë„ˆ", "ìš°ë¦¬", "ì €", "ê·¸", "ì´", "ìœ„", "ì•„ë˜", "ë•Œ", "ì¤‘", "ìˆ˜", "ë“±",
        "ê·¸ë¦¬ê³ ", "ê·¸ë˜ì„œ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ê·¸ë•Œ", "ìš”ì¦˜", "ì˜¤ëŠ˜", "ë‚´ì¼", "í˜„ì‹¤", "ëª¨ë“ ", "ì¼ìƒ", "ì†¡ë‘ë¦¬ì§¸"
    }
    return [kw.strip() for kw in keywords if kw.strip() not in stopwords and len(kw.strip()) > 1]



#í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords_okt_with_filter(text, sbert_model=None, top_k=3, threshold=0.35, verbose=True):
    #ëª…ì‚¬ ì¶”ì¶œ
    raw_nouns = okt.nouns(text)
    stopwords = {
        "ê²ƒ", "ì •ë§", "ì§„ì§œ", "ê·¸ëƒ¥", "ì´ëŸ°", "ì €ëŸ°", "ë„ˆë¬´", "ë§¤ìš°", "ì¢€", "ê±°ì˜", "ë“±", "ìˆ˜", "ë•Œ",
        "ë‚˜", "ë„ˆ", "ìš°ë¦¬", "ì €", "ê·¸", "ì´", "ìœ„", "ì•„ë˜", "ì¤‘", "ê·¸ë¦¬ê³ ", "ê·¸ë˜ì„œ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜",
        "ìš”ì¦˜", "ì˜¤ëŠ˜", "ë‚´ì¼", "ë¬´ì—‡", "ë‚¯ì„ "
    }
    abstract_keywords = {
        "ì‚¬ì´", "í†µí•´", "ê³¼ì •", "ëª¨ìŠµ", "ìƒê°", "ì´ì•¼ê¸°", "ìƒí™©", "ì‚¬ì‹¤", "ì‹œê°„", "ì¥ë©´",
        "ê²½í—˜", "ë¶€ë¶„", "ì‚¬ëŒ", "ì‚¬íšŒ", "ìì‹ ", "ì˜ë¯¸", "ì¡´ì¬", "ë‚´ìš©", "ì¤‘ì‹¬", "ì£¼ì¸ê³µ"
    }
    
    filtered_nouns = [
        kw for kw in raw_nouns
        #ë¶ˆìš©ì–´, ì¶”ìƒ í‚¤ì›Œë“œ ì œê±°
        if kw not in stopwords and kw not in abstract_keywords and len(kw.strip()) > 1
    ]
    
    freq_sorted = Counter(filtered_nouns).most_common()

    #í‚¤ì›Œë“œ ë²¡í„° - ì¹´í…Œê³ ë¦¬ ë²¡í„° ê°„ ìµœëŒ€ ìœ ì‚¬ë„ ê³„ì‚°
    #íŠ¹ì • ê¸°ì¤€(threshold)ì´ìƒì´ë©´ í‚¤ì›Œë“œ ì‚¬ìš©
    if sbert_model:
        result = []
        #í‚¤ì›Œë“œ ë³„ ì„ë² ë”© ìƒì„±
        for kw, _ in freq_sorted:
            kw_emb = get_sbert_embedding(kw)  # kw_embëŠ” GPU

            #ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ìœ ì‚¬ë„ ì¸¡ì •
            scores = []
            for ref_list in category_keywords.values():
                ref_embs = [get_sbert_embedding(r).to(kw_emb.device) for r in ref_list]
                ref_tensor = torch.stack(ref_embs)

                #ë²¡í„°ê°„ ìœ ì‚¬ë„ ê³„ì‚°
                sim = torch.matmul(kw_emb, ref_tensor.T).max().item()
                scores.append(sim)

            #ì„ê³„ê°’ ì´ìƒ í‚¤ì›Œë“œë§Œ ì„ íƒ
            if max(scores) >= threshold:#0.35 ì´ìƒì´ ë˜ë„ë¡ ì„¤ì •
                result.append((kw, max(scores)))

        #ìƒìœ„ top_kê°œ ì„ íƒ -> ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“¤ë§Œ ì¡´ì¬
        result = sorted(result, key=lambda x: x[1], reverse=True)[:top_k]
        final_keywords = [kw for kw, _ in result[:top_k]]

    else:
        final_keywords = [kw for kw, _ in freq_sorted[:top_k]]
    #í‚¤ì›Œë“œ í™•ì¸
    if verbose:
        print("í•„í„°ë§ëœ í‚¤ì›Œë“œ:", final_keywords)
    
    return final_keywords


#ì§ˆë¬¸ ìˆ˜ ë°›ê¸°
def get_question_count():
    if not os.path.exists(CONFIG_PATH) or not os.path.exists(METRICS_PATH):
        return 2
    try:
        with open(CONFIG_PATH, encoding="utf-8") as f:
            model_info = json.load(f)
        current_model = model_info[0]['model_name'] if isinstance(model_info, list) else model_info['model_name']
        with open(METRICS_PATH, encoding="utf-8") as f:
            metrics = json.load(f)
        for entry in metrics:
            if entry['model_name'] == current_model and 'q_num' in entry:
                return entry['q_num']
    except:
        pass
    return 2


#í‚¤ì›Œë“œì™€ ì˜ë¯¸ì ìœ¼ë¡œ ë¹„ìŠ·í•œ í…œí”Œë¦¿ 5ê°œ ì°¾ê¸°
def find_similar_templates_sbert(keyword, template_data, template_embeddings, sbert_model):
    keyword_emb = get_sbert_embedding(keyword)
    #í‚¤ì›Œë“œ - í…œí”Œë¦¿ ì„ë² ë”©ë“¤ê³¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    sims = torch.nn.functional.cosine_similarity(keyword_emb.unsqueeze(0), template_embeddings)
    top_indices = torch.topk(sims, k=5).indices.tolist()
    #ìƒìœ„ 5ê°œ ê°€ì ¸ì˜¤ê¸°
    return [template_data["questions"][i]["template"] for i in top_indices]

#ì–´ìƒ‰í•œ ì§ˆë¬¸ ìˆ˜
def is_template_suitable(keyword, question, template):
    # ì˜ˆ: ê°ˆë“±ì„ ì½ëŠ”ë‹¤ â†’ ì–´ìƒ‰
    if f"{keyword}ì„ ì½" in question or f"{keyword}ë¥¼ ì½" in question:
        return False
    #íŠ¹ì • í‚¤ì›Œë“œ ë‘ ë²ˆ ë°˜ë³µ ì–´ìƒ‰
    if question.count(keyword) > 1:
        return False
    #ì´ë¯¸ í…œí”Œë¦¿ì— í‚¤ì›Œë“œê°€ ë“¤ì–´ìˆëŠ” ê²½ìš°
    template_base = re.sub(r"\(í‚¤ì›Œë“œ\)", "", template)
    if keyword in template_base:
        return False
    return True


#ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜(ìµœì¢… í•¨ìˆ˜)
def generate_and_refine_questions(summary, template_data, template_embeddings, sbert_model, target_count=None, verbose=True):
    if target_count is None:
        target_count = get_question_count()
    #Okt + SBERT ê¸°ë°˜ í‚¤ì›Œë“œ 5ê°œ
    keywords = extract_keywords_okt_with_filter(summary, top_k=3, verbose=verbose)
    questions = []
    used_templates = set()
    keyword_usage = defaultdict(int)
    max_attempts = target_count * 10
    attempt = 0
    MAX_USE_PER_KEYWORD = 1


    # ğŸ‘‡ 2. ì»¤ìŠ¤í…€ í‚¤ì›Œë“œ í…œí”Œë¦¿ ì •ì˜
    custom_templates = {
        "ë‹­ì¥": "ì‘í’ˆ ì† ë‹­ì¥ì€ ì‘í’ˆ ì†ì—ì„œ ì–´ë–¤ ìƒì§•ì„ ë„ë‚˜ìš”?",
        "ìì‹¹": "ìì‹¹ì˜ ì„ íƒì€ ì˜¤ëŠ˜ë‚  ìš°ë¦¬ì—ê²Œ ì–´ë–¤ ë©”ì‹œì§€ë¥¼ ì „ë‹¬í•˜ë‚˜ìš”?",
        "ë§ˆë‹¹": "ë§ˆë‹¹ì€ ì‘í’ˆ ì†ì—ì„œ ì–´ë–¤ ì—­í• ì„ í–ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆë‚˜ìš”?"
    }

    # ğŸ‘‡ 3. ì»¤ìŠ¤í…€ í‚¤ì›Œë“œ ìš°ì„  ì ìš©
    filtered_keywords = []
    for kw in keywords:
        if kw in custom_templates and len(questions) < target_count:
            questions.append(custom_templates[kw])
            keyword_usage[kw] += 1
        else:
            filtered_keywords.append(kw)

    weights = [0.4, 0.3, 0.1, 0.1, 0.1]
    weighted_keywords = list(zip(keywords, weights))
    random.shuffle(weighted_keywords)

    while len(questions) < target_count and attempt < max_attempts:
        attempt += 1
        available_keywords = [kw for kw, _ in weighted_keywords if keyword_usage[kw] < MAX_USE_PER_KEYWORD]
        kw = random.choice(available_keywords) if available_keywords else random.choices(keywords, weights=weights, k=1)[0]
        if keyword_usage[kw] >= MAX_USE_PER_KEYWORD:
            continue
        templates = find_similar_templates_sbert(kw, template_data, template_embeddings, sbert_model)
        if not templates:
            templates = [
                "(í‚¤ì›Œë“œ)ì€ ë‹¹ì‹ ì—ê²Œ ì–´ë–¤ ì˜ë¯¸ì¸ê°€ìš”?",
                "(í‚¤ì›Œë“œ)ì„ í†µí•´ ë¬´ì—‡ì„ ëŠê¼ˆë‚˜ìš”?",
                "(í‚¤ì›Œë“œ)ì„ ë‹¤ì‹œ ë°”ë¼ë³¸ë‹¤ë©´ ì–´ë–¤ ì ì´ ë³´ì´ë‚˜ìš”?"
            ]
            force_use_fallback = True
        else:
            force_use_fallback = False
        random.shuffle(templates)
        for template in templates:
            if not force_use_fallback and template in used_templates:
                continue
            if "(í‚¤ì›Œë“œ)" not in template:
                continue
            question_raw = adjust_postposition(kw, template)
            question = clarify_subject(question_raw, kw)
            if question.count(kw) != 1:
                continue
            if not is_template_suitable(kw, question, template):
                continue
            if not force_use_fallback and question in questions:
                continue
            questions.append(question)
            used_templates.add(template)
            keyword_usage[kw] += 1
            break


    while len(questions) < target_count:
        available_fallback_kws = [kw for kw in keywords if keyword_usage[kw] < MAX_USE_PER_KEYWORD]
        if not available_fallback_kws:
            break
        fallback_kw = random.choice(available_fallback_kws)
        fallback_template = "(í‚¤ì›Œë“œ)ì€ ë‹¹ì‹ ì—ê²Œ ì–´ë–¤ ì˜ë¯¸ì¸ê°€ìš”?"
        question_raw = adjust_postposition(fallback_kw, fallback_template)
        question = clarify_subject(question_raw, fallback_kw)
        if question not in questions:
            questions.append(question)
            keyword_usage[fallback_kw] += 1

    return questions


@router.post("/predict_question")
def predict(input_data: TextInput):
    paragraph = input_data.paragraph.strip()
    if len(paragraph) < 30:
        raise HTTPException(status_code=422, detail="ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
    try:
        summary = summarize_kobart(paragraph)
        q_num = get_question_count()
        questions = generate_and_refine_questions(
            summary, template_data, template_embeddings, sbert_model, target_count=q_num
        )


        conn = get_connection()
        try:
            with conn.cursor() as cursor:
                sql = "INSERT INTO questionData (date, input, output) VALUES (%s, %s, %s)"
                cursor.execute(sql, (datetime.now().strftime("%Y-%m-%d"), paragraph, summary))
            conn.commit()
        finally:
            conn.close()

        return JSONResponse(content={
            "summary": summary,
            **{f"question{i+1}": q for i, q in enumerate(questions)}
        })

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ : {e}")
