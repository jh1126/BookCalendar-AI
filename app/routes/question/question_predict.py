from fastapi import APIRouter, HTTPException
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter, defaultdict
from transformers import (
    AutoModel, AutoTokenizer, AutoModelForSeq2SeqLM,
    PreTrainedTokenizerFast, BartForConditionalGeneration
)
from pydantic import BaseModel
import torch.nn.functional as F
import re, random, json, torch, os
from datetime import datetime
from database import get_connection
import traceback
from fastapi.responses import JSONResponse



router = APIRouter()

class TextInput(BaseModel):
    paragraph: str

okt = Okt()

CURRENT_DIR = os.path.dirname(__file__)
PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, "..", "..", ".."))

TEMPLATE_PATH = os.path.join(PROJECT_ROOT, "data", "question", "processed", "question_data.json")
CONFIG_PATH = os.path.join(PROJECT_ROOT, "app", "models", "question", "question_model_run.json")
METRICS_PATH = os.path.join(PROJECT_ROOT, "data", "question", "question_model_metrics.json")
MODELS_DIR = os.path.join(PROJECT_ROOT, "models", "question")

with open(TEMPLATE_PATH, encoding="utf-8") as f:
    template_data = json.load(f)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

category_keywords = {
    "ê°ì •íƒêµ¬": ["ê°ì •", "ëŠë‚Œ", "ë§ˆìŒ", "ìƒì‹¤", "ê¸°ì¨", "ë¶„ë…¸"],
    "ê´€ì ì „í™˜": ["ì‹œì„ ", "ë°˜ëŒ€", "ë‹¤ë¦„", "ì°¨ì´", "ê²½ê³„"],
    "ë©”íƒ€ì¸ì§€": ["ê¹¨ë‹¬ìŒ", "ì„±ì°°", "ë˜ëŒì•„ë´„", "ì¸ì§€", "ìƒê°"],
    "ë¹„íŒì ì‚¬ê³ ": ["ê°ˆë“±", "ë¬¸ì œ", "ê´€ìŠµ", "í¸ê²¬", "ë…¼ë¦¬", "í˜„ì‹¤"],
    "ìƒìƒë ¥ë°œíœ˜": ["ìš°ì£¼", "ìƒìƒ", "ê¿ˆ", "ë¯¸ë˜", "ì°½ì˜"],
    "ì‹œëŒ€ì™€ë§¥ë½": ["ê³¼ê±°", "ì—­ì‚¬", "ì‹œëŒ€", "ë¬¸í™”", "ë§¥ë½"],
    "ì‹¬ì¸µì£¼ì œíŒŒê³ ë“¤ê¸°": ["ë³¸ì§ˆ", "í•µì‹¬", "ì¤‘ì‹¬", "ì£¼ì œ", "ì˜ë¯¸"],
    "ì—°ê²°ì„±ì°¾ê¸°": ["ê´€ê³„", "ì—°ê²°", "ê´€ë ¨", "ë¹„êµ", "ìœ ì‚¬"],
    "ìœ¤ë¦¬ì ê³ ë¯¼": ["ìœ¤ë¦¬", "ì„ ì•…", "ì„ íƒ", "ê°€ì¹˜", "íŒë‹¨"],
    "ì¸ë¬¼ì‹¬ì¸µë¶„ì„": ["ì¸ë¬¼", "ì„±ê²©", "í–‰ë™", "ë™ê¸°", "ì„±ì¥"],
    "ì¥ë¥´ë¶„ì„": ["íŒíƒ€ì§€", "ì¶”ë¦¬", "ë¡œë§¨ìŠ¤", "SF", "ì„œì‚¬"],
    "ì°½ì˜ì ì¬í•´ì„": ["ì¬í•´ì„", "ë‹¤ì‹œë³´ê¸°", "ì˜ì™¸ì„±", "ë°˜ì „", "ì°½ì¡°"],
    "í•µì‹¬ê°€ì¹˜": ["ììœ ", "ì‚¬ë‘", "ì¡´ì¤‘", "ì±…ì„", "ê³µê°"],
    "í–‰ë™ìœ ë„": ["ì‹¤ì²œ", "í–‰ë™", "ë„ì „", "ì°¸ì—¬", "ë³€í™”"]
}

def load_kobart_model_and_tokenizer():
    # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
    with open(CONFIG_PATH, encoding='utf-8') as f:
        model_info = json.load(f)
    model_name = model_info[0]['model_name'] if isinstance(model_info, list) else model_info['model_name']

    # ëª¨ë¸ ê²½ë¡œ êµ¬ì„±
    model_path = os.path.join(MODELS_DIR, model_name)

    # Tokenizer + Model ë¡œë”©
    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)
    model = BartForConditionalGeneration.from_pretrained(model_path).to(device).eval()

    return tokenizer, model

model_name = "snunlp/KR-SBERT-V40K-klueNLI-augSTS"
tokenizer = AutoTokenizer.from_pretrained(model_name)
sbert_model = AutoModel.from_pretrained(model_name)
sbert_model.to("cuda")
sbert_model.eval()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def get_sbert_embedding(text):
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512
    ).to(device)  # âœ… í•˜ë“œì½”ë”©ëœ "cuda" â†’ device ë³€ìˆ˜ë¡œ í†µì¼

    with torch.no_grad():
        outputs = sbert_model(**inputs)

    cls_emb = outputs.last_hidden_state[:, 0, :]  # shape: (1, hidden_dim)
    return F.normalize(cls_emb, p=2, dim=1).squeeze(0).to(device)  # âœ… .to(device) ì¶”ê°€ë¡œ ëª…ì‹œ



# 1. (í‚¤ì›Œë“œ) ì œê±°í•œ í…œí”Œë¦¿ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
template_texts_clean = [
    q["template"].replace("(í‚¤ì›Œë“œ)", "").strip()
    for q in template_data.get("questions", [])
]

# 2. SBERT ì„ë² ë”© ì¶”ì¶œ í›„ ìŠ¤íƒ (GPUë¡œ ì´ë™)
template_embeddings = torch.stack([
    get_sbert_embedding(text) for text in template_texts_clean
]).to("cuda" if torch.cuda.is_available() else "cpu")

#ìš”ì•½ ì „ì²˜
def preprocess_paragraph(text):
    """ìš”ì•½ ì…ë ¥ìš© ë¬¸ë‹¨ ì „ì²˜ë¦¬: ì¤„ë°”ê¿ˆ ì œê±° + ê³µë°± ì •ë¦¬"""
    return ' '.join(text.strip().split())


kobart_tokenizer, kobart_model = load_kobart_model_and_tokenizer()
#ìš”ì•½ í•¨ìˆ˜
def summarize_kobart(text):
    input_ids = kobart_tokenizer.encode(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512
    ).to(device)

    output_ids = kobart_model.generate(
        input_ids,
        max_length=128,
        num_beams=4,
        early_stopping=True
    )

    return kobart_tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()

#ì¡°ì‚¬ ë³´ì • í•¨
def adjust_postposition(keyword, template):
    def has_jongseong(char):
        code = ord(char)
        return (code - 0xAC00) % 28 != 0 if 0xAC00 <= code <= 0xD7A3 else False

    has_final = has_jongseong(keyword[-1])
    replacements = {
        r"\(ì´\)ê°€": "ì´" if has_final else "ê°€",
        r"\(ì„\)ë¥¼": "ì„" if has_final else "ë¥¼",
        r"\(ì€\)ëŠ”": "ì€" if has_final else "ëŠ”",
        r"\(ê³¼\)ì™€": "ê³¼" if has_final else "ì™€",
        r"\(ì´\)ë¼ê³ ": "ì´ë¼ê³ " if has_final else "ë¼ê³ ",
    }

    for pattern, repl in replacements.items():
        template = re.sub(pattern, repl, template)
    return template.replace("(í‚¤ì›Œë“œ)", keyword).strip()

# ì£¼ì–´ ë³´ì • í•¨ìˆ˜
def clarify_subject(question, keyword):
    abstract_keywords = {"ê°ì •", "ë‚´ë©´", "ì˜ë¯¸", "ìƒê°", "ì¡´ì¬", "ì„±ì°°", "ëŠë‚Œ"}
    human_keywords = {"ì €ì", "ì£¼ì¸ê³µ", "ì¸ë¬¼", "ì‚¬ëŒ"}

    replacement = None

    if any(phrase in question for phrase in ["í•˜ê³  ì‹¶ì–´", "ë§Œë“¤ê³  ì‹¶ì–´", "ëŠë¼ê³  ì‹¶ì–´", "ìƒê°í•˜ë‚˜ìš”", "ì¤‘ìš”í•œê°€ìš”"]):
        if keyword in abstract_keywords:
            replacement = f"ì €ìì˜ {keyword}"
        elif keyword in human_keywords:
            replacement = f"{keyword} ë³¸ì¸ì€"

    # ë‹¨ì–´ê°€ í¬í•¨ëœ ê²½ìš°, ì²˜ìŒ í•œ ë²ˆë§Œ ëŒ€ì²´
    if replacement:
        question = re.sub(rf"\b{re.escape(keyword)}\b", replacement, question, count=1)

    # "ë¬´ì—‡ì¸ê°€ìš”?" â†’ "ì–´ë–¤ ì˜ë¯¸ì¸ê°€ìš”?" ë¡œ ë” ìì—°ìŠ¤ëŸ½ê²Œ
    if "ë¬´ì—‡ì¸ê°€ìš”?" in question and keyword in abstract_keywords:
        question = question.replace("ë¬´ì—‡ì¸ê°€ìš”?", "ì–´ë–¤ ì˜ë¯¸ì¸ê°€ìš”?")

    return question


# 5-2. ë¶ˆìš©ì–´ ì œê±°
def clean_keywords(keywords):
    stopwords = {
        "ê²ƒ", "ì •ë§", "ì§„ì§œ", "ê·¸ëƒ¥", "ì´ëŸ°", "ì €ëŸ°", "ë„ˆë¬´", "ë§¤ìš°", "ì¢€", "ê±°ì˜",
        "ë‚˜", "ë„ˆ", "ìš°ë¦¬", "ì €", "ê·¸", "ì´", "ìœ„", "ì•„ë˜", "ë•Œ", "ì¤‘", "ìˆ˜", "ë“±",
        "ê·¸ë¦¬ê³ ", "ê·¸ë˜ì„œ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ê·¸ë•Œ", "ìš”ì¦˜", "ì˜¤ëŠ˜", "ë‚´ì¼"
    }
    return [kw.strip() for kw in keywords if kw.strip() not in stopwords and len(kw.strip()) > 1]

def extract_keywords_okt_with_filter(text, sbert_model=None, top_k=10, threshold=0.35, verbose=True):
    raw_nouns = okt.nouns(text)
    stopwords = {
        "ê²ƒ", "ì •ë§", "ì§„ì§œ", "ê·¸ëƒ¥", "ì´ëŸ°", "ì €ëŸ°", "ë„ˆë¬´", "ë§¤ìš°", "ì¢€", "ê±°ì˜", "ë“±", "ìˆ˜", "ë•Œ",
        "ë‚˜", "ë„ˆ", "ìš°ë¦¬", "ì €", "ê·¸", "ì´", "ìœ„", "ì•„ë˜", "ì¤‘", "ê·¸ë¦¬ê³ ", "ê·¸ë˜ì„œ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜",
        "ìš”ì¦˜", "ì˜¤ëŠ˜", "ë‚´ì¼"
    }
    abstract_keywords = {
        "ì‚¬ì´", "í†µí•´", "ê³¼ì •", "ëª¨ìŠµ", "ìƒê°", "ì´ì•¼ê¸°", "ìƒí™©", "ì‚¬ì‹¤", "ì‹œê°„", "ì¥ë©´",
        "ê²½í—˜", "ë¶€ë¶„", "ì‚¬ëŒ", "ì‚¬íšŒ", "ìì‹ ", "ì˜ë¯¸", "ì¡´ì¬", "ë‚´ìš©", "ì¤‘ì‹¬", "ì£¼ì¸ê³µ"
    }
    
    filtered_nouns = [
        kw for kw in raw_nouns
        if kw not in stopwords and kw not in abstract_keywords and len(kw.strip()) > 1
    ]
    
    freq_sorted = Counter(filtered_nouns).most_common()
    
    if sbert_model:
        result = []
        for kw, _ in freq_sorted:
            kw_emb = get_sbert_embedding(kw)  # kw_embëŠ” GPU

            scores = []
            for ref_list in category_keywords.values():
                ref_embs = [get_sbert_embedding(r).to(kw_emb.device) for r in ref_list]
                ref_tensor = torch.stack(ref_embs)


                sim = torch.matmul(kw_emb, ref_tensor.T).max().item()
                scores.append(sim)

            if max(scores) >= threshold:
                result.append((kw, max(scores)))

        # âœ… ë£¨í”„ ë°”ê¹¥ìœ¼ë¡œ ì˜®ê¹€
        result = sorted(result, key=lambda x: x[1], reverse=True)[:top_k]
        final_keywords = [kw for kw, _ in result]

    else:
        final_keywords = [kw for kw, _ in freq_sorted[:top_k]]
    
    if verbose:
        print("ğŸ“Œ í•„í„°ë§ëœ í‚¤ì›Œë“œ:", final_keywords)
    
    return final_keywords


#ì§ˆë¬¸ ìˆ˜ ë°›ê¸°
def get_question_count():
    if not os.path.exists(CONFIG_PATH) or not os.path.exists(METRICS_PATH):
        return 2
    try:
        with open(CONFIG_PATH, encoding="utf-8") as f:
            model_info = json.load(f)
        current_model = model_info[0]['model_name'] if isinstance(model_info, list) else model_info['model_name']
        with open(METRICS_PATH, encoding="utf-8") as f:
            metrics = json.load(f)
        for entry in metrics:
            if entry['model_name'] == current_model and 'q_num' in entry:
                return entry['q_num']
    except:
        pass
    return 2

#ë¹„ìŠ·í•œ ì§ˆë¬¸ í…œí”Œë¦¿ ì°¾ê¸° -sbert
def find_similar_templates_sbert(keyword, template_data, template_embeddings, sbert_model):
    keyword_emb = get_sbert_embedding(keyword)
    sims = torch.nn.functional.cosine_similarity(keyword_emb.unsqueeze(0), template_embeddings)
    top_indices = torch.topk(sims, k=5).indices.tolist()
    return [template_data["questions"][i]["template"] for i in top_indices]

#ì–´ìƒ‰í•œ ì§ˆë¬¸ ìˆ˜
def is_template_suitable(keyword, question):
    # ì˜ˆ: ê°ˆë“±ì„ ì½ëŠ”ë‹¤ â†’ ì–´ìƒ‰
    if f"{keyword}ì„ ì½" in question or f"{keyword}ë¥¼ ì½" in question:
        return False
    if question.count(keyword) > 1 and keyword in {"ê°ˆë“±", "ê°ì •", "ê¸°íšŒ"}:
        return False
    return True


#ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜
def generate_and_refine_questions(summary, template_data, template_embeddings, sbert_model, target_count=None, verbose=True):
    if target_count is None:
        target_count = get_question_count()
    keywords = extract_keywords_okt_with_filter(summary, top_k=5, verbose=verbose)
    questions = []
    used_templates = set()
    keyword_usage = defaultdict(int)
    max_attempts = target_count * 10
    attempt = 0
    MAX_USE_PER_KEYWORD = 1

    weights = [0.4, 0.3, 0.1, 0.1, 0.1]
    weighted_keywords = list(zip(keywords, weights))
    random.shuffle(weighted_keywords)

    while len(questions) < target_count and attempt < max_attempts:
        attempt += 1
        available_keywords = [kw for kw, _ in weighted_keywords if keyword_usage[kw] < MAX_USE_PER_KEYWORD]
        kw = random.choice(available_keywords) if available_keywords else random.choices(keywords, weights=weights, k=1)[0]
        if keyword_usage[kw] >= MAX_USE_PER_KEYWORD:
            continue
        templates = find_similar_templates_sbert(kw, template_data, template_embeddings, sbert_model)
        if not templates:
            templates = [
                "(í‚¤ì›Œë“œ)ì€ ë‹¹ì‹ ì—ê²Œ ì–´ë–¤ ì˜ë¯¸ì¸ê°€ìš”?",
                "(í‚¤ì›Œë“œ)ì„ í†µí•´ ë¬´ì—‡ì„ ëŠê¼ˆë‚˜ìš”?",
                "(í‚¤ì›Œë“œ)ì„ ë‹¤ì‹œ ë°”ë¼ë³¸ë‹¤ë©´ ì–´ë–¤ ì ì´ ë³´ì´ë‚˜ìš”?"
            ]
            force_use_fallback = True
        else:
            force_use_fallback = False
        random.shuffle(templates)
        for template in templates:
            if not force_use_fallback and template in used_templates:
                continue
            if "(í‚¤ì›Œë“œ)" not in template:
                continue
            question_raw = adjust_postposition(kw, template)
            question = clarify_subject(question_raw, kw)
            if question.count(kw) != 1:
                continue
            if not is_template_suitable(kw, question):
                continue
            if not force_use_fallback and question in questions:
                continue
            questions.append(question)
            used_templates.add(template)
            keyword_usage[kw] += 1
            break

    while len(questions) < target_count:
        available_fallback_kws = [kw for kw in keywords if keyword_usage[kw] < MAX_USE_PER_KEYWORD]
        if not available_fallback_kws:
            break
        fallback_kw = random.choice(available_fallback_kws)
        fallback_template = "(í‚¤ì›Œë“œ)ì€ ë‹¹ì‹ ì—ê²Œ ì–´ë–¤ ì˜ë¯¸ì¸ê°€ìš”?"
        question_raw = adjust_postposition(fallback_kw, fallback_template)
        question = clarify_subject(question_raw, fallback_kw)
        if question not in questions:
            questions.append(question)
            keyword_usage[fallback_kw] += 1

    return questions


@router.post("/predict_question")
def predict(input_data: TextInput):
    paragraph = input_data.paragraph.strip()
    if len(paragraph) < 30:
        raise HTTPException(status_code=422, detail="ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
    try:
        summary = summarize_kobart(paragraph)
        q_num = get_question_count()
        questions = generate_and_refine_questions(
            summary, template_data, template_embeddings, sbert_model, target_count=q_num
        )


        conn = get_connection()
        try:
            with conn.cursor() as cursor:
                sql = "INSERT INTO questionData (date, input, output) VALUES (%s, %s, %s)"
                cursor.execute(sql, (datetime.now().strftime("%Y-%m-%d"), paragraph, summary))
            conn.commit()
        finally:
            conn.close()

        return JSONResponse(content={
            "summary": summary,
            **{f"question{i+1}": q for i, q in enumerate(questions)}
        })

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ : {e}")
